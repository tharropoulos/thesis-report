% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{githubcopilot}
\BIBentryALTinterwordspacing
``Github copilot.'' [Online]. Available: \url{https://github.com/features/copilot}
\BIBentrySTDinterwordspacing

\bibitem{bommasani2021opportunities}
R.~Bommasani, D.~A. Hudson, E.~Adeli, R.~Altman, S.~Arora, S.~von Arx, M.~S. Bernstein, J.~Bohg, A.~Bosselut, E.~Brunskill \emph{et~al.}, ``On the opportunities and risks of foundation models,'' 2021.

\bibitem{zhao2023survey}
W.~X. Zhao, K.~Zhou, J.~Li, T.~Tang, X.~Wang, Y.~Hou, Y.~Min, B.~Zhang, J.~Zhang, Z.~Dong \emph{et~al.}, ``A survey of large language models,'' 2023.

\bibitem{zhou2023comprehensive}
Q.~Zhou, C.~Li, J.~Yu, Y.~Liu, G.~Wang, K.~Zhang, C.~Ji, Q.~Yan, L.~He \emph{et~al.}, ``A comprehensive survey on pretrained foundation models: A history from bert to chatgpt,'' \emph{arXiv preprint arXiv:2302.09419}, 2023.

\bibitem{openai2022chatgpt}
\BIBentryALTinterwordspacing
OpenAI, ``Chatgpt: Optimizing language models for dialogue,'' 2022. [Online]. Available: \url{https://web.archive.org/web/20221130180912/https://openai.com/blog/chatgpt/}
\BIBentrySTDinterwordspacing

\bibitem{guardian2024openai}
K.~Staff, ``Microsoft-backed openai valued at \$80bn after company completes deal,'' \emph{The Guardian}, Feb 2024, retrieved May 30, 2024.

\bibitem{nyt2024openai}
C.~Metz and T.~Mickle, ``Openai completes deal that values the company at \$80 billion,'' \emph{The New York Times}, Feb 2024, retrieved May 30, 2024.

\bibitem{verge2023chatgpt}
E.~Roth, ``Microsoft spent hundreds of millions of dollars on a chatgpt supercomputer,'' \emph{The Verge}, Mar 2023, archived from the original on May 30, 2023. Retrieved March 30, 2023.

\bibitem{liu2023chatgpt}
Z.~Liu, ``Chatgpt will command more than 30,000 nvidia gpus: Report,'' \emph{Tom's Hardware}, Mar 2023, archived from the original on May 30, 2024. Retrieved November 2, 2023.

\bibitem{trendforce2023ai}
``Trendforce says with cloud companies initiating ai arms race, gpu demand from chatgpt could reach 30,000 chips as it readies for commercialization,'' \emph{TrendForce}, Nov 2023, archived from the original on May 30, 2024. Retrieved November 2, 2023.

\bibitem{adams2012mapping}
S.~Adams, I.~Arel, J.~Bach, R.~Coop, R.~Furlan, B.~Goertzel, J.~S. Hall, A.~Samsonovich, M.~Scheutz, M.~Schlesinger \emph{et~al.}, ``Mapping the landscape of human-level artificial general intelligence,'' \emph{AI Magazine}, vol.~33, no.~1, pp. 25--42, 2012.

\bibitem{goertzel2014agi}
B.~Goertzel, ``Artificial general intelligence: Concept, state of the art, and future prospects,'' \emph{Journal of Artificial General Intelligence}, vol.~5, no.~1, pp. 1--46, 2014, submitted 2013-2-12, Accepted 2014-3-15.

\bibitem{github2021copilot}
\BIBentryALTinterwordspacing
GitHub, ``Introducing github copilot: your ai pair programmer,'' \emph{GitHub Blog}, June 2021, retrieved May 30, 2024. [Online]. Available: \url{https://github.blog/2021-06-29-introducing-github-copilot-ai-get-code-done-faster/}
\BIBentrySTDinterwordspacing

\bibitem{chen2021evaluating}
M.~Chen, J.~Tworek, H.~Jun, Q.~Yuan, H.~P. de~Oliveira~Pinto, J.~Kaplan, H.~Edwards, Y.~Burda, N.~Joseph, G.~Brockman, A.~Ray, R.~Puri, G.~Krueger, M.~Petrov, H.~Khlaaf, G.~Sastry, P.~Mishkin, B.~Chan, S.~Gray, N.~Ryder, M.~Pavlov, A.~Power, L.~Kaiser, M.~Bavarian, C.~Winter, P.~Tillet, F.~P. Such, D.~Cummings, M.~Plappert, F.~Chantzis, E.~Barnes, A.~Herbert-Voss, W.~H. Guss, A.~Nichol, A.~Paino, N.~Tezak, J.~Tang, I.~Babuschkin, S.~Balaji, S.~Jain, W.~Saunders, C.~Hesse, A.~N. Carr, J.~Leike, J.~Achiam, V.~Misra, E.~Morikawa, A.~Radford, M.~Knight, M.~Brundage, M.~Murati, K.~Mayer, P.~Welinder, B.~McGrew, D.~Amodei, S.~McCandlish, I.~Sutskever, and W.~Zaremba, ``Evaluating large language models trained on code,'' 2021.

\bibitem{kemper2023openai}
\BIBentryALTinterwordspacing
J.~Kemper. (2023, March) Openai kills its codex code model, recommends gpt3.5 instead. Retrieved May 27, 2024. [Online]. Available: \url{https://thedecoder.com/openai-kills-its-codex-code-model-recommends-gpt3-5-instead/}
\BIBentrySTDinterwordspacing

\bibitem{github2023copilotupdate}
\BIBentryALTinterwordspacing
S.~Z. Zhao, ``Smarter, more efficient coding: Github copilot goes beyond codex with improved ai model,'' \emph{GitHub Blog}, July 2023, retrieved May 27, 2024. [Online]. Available: \url{https://github.blog/2023-07-28-smarter-more-efficient-coding-github-copilot-goes-beyond-codex-with-improved-ai-model/}
\BIBentrySTDinterwordspacing

\end{thebibliography}
